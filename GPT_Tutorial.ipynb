{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XtraBunger/XtraBunger.github.io/blob/main/GPT_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stormhacks 2024 GPT Tutorial\n",
        "\n",
        "**IMPORTANT:** Make a copy of this notebook so you can edit and run it! Go to **File > Save a copy in Drive**. You need a Gmail account to do this.\n",
        "\n",
        "This notebook covers how to setup an OpenAI account and configuring your programs to access the OpenAI API. It also covers how to use their Chat Completion feature with a few examples and also some tips on prompt engineering.\n",
        "\n",
        "Along the way, you can refer to the OpenAI references:\n",
        "\n",
        "- **Documentation:** https://platform.openai.com/docs/\n",
        "- **API Reference:** https://platform.openai.com/docs/api-reference\n",
        "\n",
        "You do not need to install anything to run this notebook. However, we will be doing some Python programming, so knowledge of Python will be helpful to follow along."
      ],
      "metadata": {
        "id": "lzyT0EDjmFHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background"
      ],
      "metadata": {
        "id": "Me1lSMK-s1sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is GPT?\n",
        "\n",
        "GPT stands for \"Generative Pre-trained Transformers\", and it is a Large Language Model (LLM) that is capable of text generation. GPT generates text in response to an input, often called a \"prompt\".\n",
        "\n",
        "What is GPT able to do? Some examples are:\n",
        "- Text analysis\n",
        "- Writing code and debugging\n",
        "- Language translation\n",
        "- Answering questions about a particular topic\n",
        "- Surfing the web (Bing Chat)\n",
        "\n",
        "The OpenAI documentation has a more comprehensive list on [example usecases of GPT](https://platform.openai.com/examples)."
      ],
      "metadata": {
        "id": "1y4Kdkevs8j-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the OpenAI API?\n",
        "\n",
        "An API (Application Programming Interface) is a way for two computer programs to communicate with each other and exchange data. Simply put, the OpenAI API is a protocol that allows you to utilize their software to perform various tasks, like using GPT to perform text generation.\n",
        "\n",
        "The OpenAI API supports more than just text generation. It also provides features that allows you to tasks related to vision and speech.\n",
        "\n",
        "This tutorial only focuses on text generation. Check out the [OpenAI documentation](https://platform.openai.com/docs/guides/text-generation) to see the full capabilities of their text generation API."
      ],
      "metadata": {
        "id": "MedWo_H5zEDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI API Setup"
      ],
      "metadata": {
        "id": "LcYkwBApx1-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an Account with OpenAI\n",
        "\n",
        "To begin, you should [create an account with OpenAI](https://platform.openai.com/login/) if you have not already. When creating a new account, you are provided with free credits to use GPT-3 models for your project!\n",
        "\n",
        "### Create a New Project\n",
        "\n",
        "You will notice that the current project is the \"Default Project\". It is recommended to create a new project so you can invite all your team members so that everybody can collaborate on the same thing.\n",
        "\n",
        "To create a new project, click on your profile icon in the top left corner and click **Create project +**. Give your project a name and then click **create**!\n",
        "\n",
        "In the project settings, you can also invite your team members to collaborate.\n",
        "\n",
        "### Generate an API Key\n",
        "\n",
        "An API key associated with your account is required to use their products in your project. Open the ðŸ”’**API keys** tab on the sidebar and click **+ Create new secret key**. Be sure to save your key somewhere safe!\n",
        "\n",
        "Copy and paste your key below ðŸ‘‡"
      ],
      "metadata": {
        "id": "EtBcn9OBx_rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"\""
      ],
      "metadata": {
        "id": "zRJW2XBTmL4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the OpenAI API\n",
        "\n",
        "The setup steps are dependent on the tools that you are using. The [OpenAI documentation](https://platform.openai.com/docs/quickstart/quickstart-language-selection) covers how to install the API for Python or Node.js.\n",
        "\n",
        "This notebook uses Python, so we will be following the instructions for Python. Note that the installation step should be completed in your terminal when developing locally on your computer."
      ],
      "metadata": {
        "id": "i8370PR_rd4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "D81NDowDsF8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start the OpenAI Client"
      ],
      "metadata": {
        "id": "sUWTCBMpzy31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "BUaOGyi5z5pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Completions API\n",
        "\n",
        "When using the API, the main tool that will be used here to generate text is the [`client.chat.completions.create`](https://platform.openai.com/docs/api-reference/chat) function. There are two arguments that are required:\n",
        "\n",
        "- `model` - The string ID of the model the use for text generation.\n",
        "- `messages` - The list of messages comprising the conversation so far.\n",
        "\n",
        "Each message is formatted as a dictionary (or an associative array in JavaScript):\n",
        "\n",
        "- `content` - The contents of the message\n",
        "- `role` - The role of the messages author.\n",
        "    - `\"system\"` -  The background information and instructions given to the model.\n",
        "    - `\"user\"` -  The prompt and/or information provided by the user of the application.\n",
        "    - `\"assistant\"` -  The response from the assistant.\n",
        "    - `\"tool\"` -  The response from a function call. Learn more about [function calls](https://platform.openai.com/docs/guides/function-calling).\n",
        "- `name` - An optional name to provide the message to differentiate between other messages of the same role."
      ],
      "metadata": {
        "id": "0VnUY7634Ezx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Performing a Simple Task\n",
        "\n",
        "Let's use text completion to translate a sentence in English to a specified language."
      ],
      "metadata": {
        "id": "MYE4S2iM4UBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text: str, language: str) -> str:\n",
        "  \"\"\"Translates text into the specified language.\"\"\"\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          # FILL THE CONTENT STRING WITH A SYSTEM MESSAGE\n",
        "          {\"role\": \"system\", \"content\": f\"\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "      ]\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "2f_RRol0zlYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_text = translate(text=\"\", language=\"\")\n",
        "print(translated_text)"
      ],
      "metadata": {
        "id": "hkkFKaDpB8pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Creating a Chatbot\n",
        "\n",
        "Let's create a simple helpful assistant! Given that the chatbot should remember the conversation, let's give it the Fibonacci Sequence with each number in a separate message. It should remember the sequence and hopefully predict the next number.\n",
        "\n",
        "Sequence: 0, 1, 1, 2, 3, 5, 8, 13"
      ],
      "metadata": {
        "id": "VXnnCFuRChXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(user_prompt: str, past_messages: list[dict]) -> str:\n",
        "  \"\"\"Get a response from the chatbot.\"\"\"\n",
        "  prompt_message = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=past_messages + prompt_message\n",
        "  )\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "def chat():\n",
        "  \"\"\"Chat with the chatbot.\"\"\"\n",
        "\n",
        "  # Initialize the conversation with the system context\n",
        "\n",
        "  # Facilitate the conversation until it's done\n",
        "  print(\"Starting the conversation...\")\n",
        "  while True:\n",
        "\n",
        "    # Get the user input\n",
        "\n",
        "    # If prompt == \"q\", stop the conversation\n",
        "\n",
        "    # Else... Get the response, update the converation, and print the response\n",
        "\n",
        "      # Get the response\n",
        "\n",
        "      # Update the conversation with the user prompt and response\n",
        "\n",
        "      # Print the response from the chatbot"
      ],
      "metadata": {
        "id": "Ud5WEvazCmWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To quit the conversation: Enter \"q\"\n",
        "chat()"
      ],
      "metadata": {
        "id": "nNTUC5_vEdm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Playground\n",
        "\n",
        "Before using the OpenAI API for your project, I recomend checking out the [OpenAI Playground](https://platform.openai.com/playground/). It will be useful for prototyping through a UI before integration into your application. In particular, it is useful for testing out both simple chat completions and creating chatbots."
      ],
      "metadata": {
        "id": "5wKJgQexKby0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering\n",
        "\n",
        "\"Prompt Engineering\" is the act of designing effective prompts to instruct an LLM to perform a desired task. The quality of the prompt significantly affects the output of LLMs.\n",
        "\n",
        "OpenAI provides a [Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering), but some of the material is covered in the sections that follow in this notebook.\n",
        "\n",
        "The first few examples are based on Figure 1 from [here](https://arxiv.org/pdf/2201.11903)."
      ],
      "metadata": {
        "id": "83f4omgF3mEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot Prompting\n",
        "\n",
        "This is by far one of the most common types of prompts. A zero-shot prompt means to ask the question directly. This typically means that you hope that the LLM will reason about what the answer should be on its own without any prior experience or examples.\n",
        "\n",
        "**Example**\n",
        "\n",
        "---\n",
        "\n",
        "_Prompt_\n",
        "\n",
        "Q: Alice went to the grocery store and bought 4 bags of apples and 3 bags of oranges. Each bag contains 7 pieces of fruit. How many pieces of fruit did Alice buy in total?\n",
        "\n",
        "---\n",
        "\n",
        "_Response_\n",
        "\n",
        "The answer is ___"
      ],
      "metadata": {
        "id": "CgiHrZyW4qa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Few-shot prompting provides the LLM with previous examples before asking it the question that we want it to answer. The idea is that providing previous examples should help the LLM learn by example and give it more reasoning capabilities for the task at hand.\n",
        "\n",
        "**Example**\n",
        "\n",
        "---\n",
        "\n",
        "_Prompt_\n",
        "\n",
        "Q: Bob has 3 sponges. He goes to the store to get 2 more bags of sponges with\n",
        "each bag containing 5 sponges. How many sponges does Bob have now?\n",
        "\n",
        "A: The answer is 13\n",
        "\n",
        "Q: Alice went to the grocery store and bought 4 bags of apples and 3 bags of oranges. Each bag contains 7 pieces of fruit. How many pieces of fruit did Alice buy in total?\n",
        "\n",
        "---\n",
        "\n",
        "_Response_\n",
        "\n",
        "The answer is ___"
      ],
      "metadata": {
        "id": "SI3YGx2u6mob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain-of-Thought Prompting\n",
        "\n",
        "You might sometimes hear referred to ask \"giving the model time to think\". This builds off few-shot prompting by also explaining the thought process that lead to the answer. This can provide the LLM with powerful reasoning capabilities to perform complex tasks.\n",
        "\n",
        "**Example**\n",
        "\n",
        "---\n",
        "\n",
        "_Prompt_\n",
        "\n",
        "Q: Bob has 3 sponges. He goes to the store to get 2 more bags of sponges with\n",
        "each bag containing 5 sponges. How many sponges does Bob have now?\n",
        "\n",
        "A: Bob starts with 3 sponges. Since each bag has 5 sponges and he bought 2 bags, he bought an additional 10 sponges. Since 3 + 10 = 13, the answer is 13.\n",
        "\n",
        "Q: Alice went to the grocery store and bought 4 bags of apples and 3 bags of oranges. Each bag contains 7 pieces of fruit. How many pieces of fruit did Alice buy in total?\n",
        "\n",
        "---\n",
        "\n",
        "_Response_\n",
        "\n",
        "The answer is ___"
      ],
      "metadata": {
        "id": "29275tc38y1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write Clear Instructions\n",
        "\n",
        "Clarity is better understood by example.\n",
        "\n",
        "Let's say we wanted to build a sentiment classifier that classifies text as having either a \"Positive\" or \"Negative\" sentiment. The desired output should be \"Positive\" or \"Negative\" and nothing else."
      ],
      "metadata": {
        "id": "cL1LKlwmAaZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(text: str) -> str:\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      # Provide the context to the system\n",
        "      {\"role\": \"system\", \"content\": \"You are an assistant that identifies whether a sentence has a positive sentiment or not.\"},\n",
        "\n",
        "      # Provide the text to classify\n",
        "      {\"role\": \"user\", \"content\": text}\n",
        "    ]\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "pbouysj6WV-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love computer vision. Getting computers to see is amazing!\"\n",
        "print(classify_text(text))"
      ],
      "metadata": {
        "id": "OdmHWwI8Wt29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above text (hopefully, for demonstration) does not give the desired output. It probably gives some short blurb saying that the sentence has a positive sentiment, but not the word \"Positive\" by itself.\n",
        "\n",
        "We can tweak the system message to be more specific about what format we want in the output, which should hopefully solve the problem."
      ],
      "metadata": {
        "id": "nHDPudy4W96M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(text: str) -> str:\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      # Provide the context to the system\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are an assistant that identifies whether a sentence has a positive sentiment or not.\" +\\\n",
        "                     \"The output can only be either \\\"Positive\\\" or \\\"Negative\\\", and nothing else.\"\n",
        "      },\n",
        "\n",
        "      # Provide the text to translate as a user prompt\n",
        "      {\"role\": \"user\", \"content\": text}\n",
        "    ]\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "VbW9SmAEXr7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love computer vision. Getting computers to see is amazing!\"\n",
        "print(classify_text(text))"
      ],
      "metadata": {
        "id": "7zMqjHq7YQWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Resume Reviewer\n",
        "\n",
        "Let's make a resume reviewer that takes a resume bullet point and provides suggestions on how to improve it. A few ways to make improvements are:\n",
        "\n",
        "1. Starting each point with an impactful verb\n",
        "2. Impact should be numerically quantified where possible and should be indicated closer to the beginning of the bullet point\n",
        "3. The resume is for a software engineering position\n",
        "\n",
        "The input is the bullet point to be improved. The model should list up to three suggestions for improvement as the output.\n",
        "\n",
        "In particular, we want to try to use **few-shot prompting** and **clear instructions** to perform this task effectively."
      ],
      "metadata": {
        "id": "EFsY-jQQELEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_MESSAGE =\\\n",
        "\"\"\"\n",
        "Write your system prompt here...\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Qv_2Xl90Gq1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suggestions(system_message: str, user_bullet_point: str, num_suggestions: int) -> str:\n",
        "    completion = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "        # Provide the context to the system\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "\n",
        "        # Provide the text from the user\n",
        "        {\"role\": \"user\", \"content\": user_bullet_point},\n",
        "      ],\n",
        "\n",
        "      # Tell the model to generate `num_suggestions` suggestions\n",
        "      n=num_suggestions,\n",
        "\n",
        "      # Higher temperature = more random results\n",
        "      temperature=1.5\n",
        "    )\n",
        "    return [suggestion.message.content for suggestion in completion.choices]"
      ],
      "metadata": {
        "id": "Fs2PLCJqG6h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Your bullet point here\n",
        "BULLET_POINT = \"\"\n",
        "####\n",
        "\n",
        "suggestions = get_suggestions(\n",
        "    system_message=SYSTEM_MESSAGE,\n",
        "    user_bullet_point=BULLET_POINT,\n",
        "    num_suggestions=3\n",
        ")\n",
        "\n",
        "for idx, suggestion in enumerate(suggestions):\n",
        "  print(f\"{idx+1}. {suggestion}\")"
      ],
      "metadata": {
        "id": "HPw5i_TPIwYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}